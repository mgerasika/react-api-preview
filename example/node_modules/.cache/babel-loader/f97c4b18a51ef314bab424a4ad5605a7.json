{"ast":null,"code":"import { __extends } from \"tslib\";\nimport { CreateDeliveryStreamInput, CreateDeliveryStreamOutput } from \"../models/models_0\";\nimport { deserializeAws_json1_1CreateDeliveryStreamCommand, serializeAws_json1_1CreateDeliveryStreamCommand } from \"../protocols/Aws_json1_1\";\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\n/**\n * <p>Creates a Kinesis Data Firehose delivery stream.</p>\n *\n *          <p>By default, you can create up to 50 delivery streams per AWS Region.</p>\n *          <p>This is an asynchronous operation that immediately returns. The initial status of the\n *          delivery stream is <code>CREATING</code>. After the delivery stream is created, its status\n *          is <code>ACTIVE</code> and it now accepts data. If the delivery stream creation fails, the\n *          status transitions to <code>CREATING_FAILED</code>. Attempts to send data to a delivery\n *          stream that is not in the <code>ACTIVE</code> state cause an exception. To check the state\n *          of a delivery stream, use <a>DescribeDeliveryStream</a>.</p>\n *          <p>If the status of a delivery stream is <code>CREATING_FAILED</code>, this status\n *          doesn't change, and you can't invoke <code>CreateDeliveryStream</code> again on it.\n *          However, you can invoke the <a>DeleteDeliveryStream</a> operation to delete\n *          it.</p>\n *          <p>A Kinesis Data Firehose delivery stream can be configured to receive records directly\n *          from providers using <a>PutRecord</a> or <a>PutRecordBatch</a>, or it\n *          can be configured to use an existing Kinesis stream as its source. To specify a Kinesis\n *          data stream as input, set the <code>DeliveryStreamType</code> parameter to\n *             <code>KinesisStreamAsSource</code>, and provide the Kinesis stream Amazon Resource Name\n *          (ARN) and role ARN in the <code>KinesisStreamSourceConfiguration</code>\n *          parameter.</p>\n *          <p>To create a delivery stream with server-side encryption (SSE) enabled, include <a>DeliveryStreamEncryptionConfigurationInput</a> in your request. This is\n *          optional. You can also invoke <a>StartDeliveryStreamEncryption</a> to turn on\n *          SSE for an existing delivery stream that doesn't have SSE enabled.</p>\n *          <p>A delivery stream is configured with a single destination: Amazon S3, Amazon ES,\n *          Amazon Redshift, or Splunk. You must specify only one of the following destination\n *          configuration parameters: <code>ExtendedS3DestinationConfiguration</code>,\n *             <code>S3DestinationConfiguration</code>,\n *             <code>ElasticsearchDestinationConfiguration</code>,\n *             <code>RedshiftDestinationConfiguration</code>, or\n *             <code>SplunkDestinationConfiguration</code>.</p>\n *          <p>When you specify <code>S3DestinationConfiguration</code>, you can also provide the\n *          following optional values: BufferingHints, <code>EncryptionConfiguration</code>, and\n *             <code>CompressionFormat</code>. By default, if no <code>BufferingHints</code> value is\n *          provided, Kinesis Data Firehose buffers data up to 5 MB or for 5 minutes, whichever\n *          condition is satisfied first. <code>BufferingHints</code> is a hint, so there are some\n *          cases where the service cannot adhere to these conditions strictly. For example, record\n *          boundaries might be such that the size is a little over or under the configured buffering\n *          size. By default, no encryption is performed. We strongly recommend that you enable\n *          encryption to ensure secure data storage in Amazon S3.</p>\n *\n *          <p>A few notes about Amazon Redshift as a destination:</p>\n *          <ul>\n *             <li>\n *                <p>An Amazon Redshift destination requires an S3 bucket as intermediate location.\n *                Kinesis Data Firehose first delivers data to Amazon S3 and then uses\n *                   <code>COPY</code> syntax to load data into an Amazon Redshift table. This is\n *                specified in the <code>RedshiftDestinationConfiguration.S3Configuration</code>\n *                parameter.</p>\n *\n *             </li>\n *             <li>\n *                <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be\n *                specified in <code>RedshiftDestinationConfiguration.S3Configuration</code> because\n *                the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't\n *                support these compression formats.</p>\n *             </li>\n *             <li>\n *                <p>We strongly recommend that you use the user name and password you provide\n *                exclusively with Kinesis Data Firehose, and that the permissions for the account are\n *                restricted for Amazon Redshift <code>INSERT</code> permissions.</p>\n *\n *             </li>\n *          </ul>\n *          <p>Kinesis Data Firehose assumes the IAM role that is configured as part of the\n *          destination. The role should allow the Kinesis Data Firehose principal to assume the role,\n *          and the role should have permissions that allow the service to deliver the data. For more\n *          information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3\">Grant Kinesis Data\n *             Firehose Access to an Amazon S3 Destination</a> in the <i>Amazon Kinesis Data\n *             Firehose Developer Guide</i>.</p>\n */\nvar CreateDeliveryStreamCommand = /** @class */function (_super) {\n  __extends(CreateDeliveryStreamCommand, _super);\n  // Start section: command_properties\n  // End section: command_properties\n  function CreateDeliveryStreamCommand(input) {\n    var _this =\n    // Start section: command_constructor\n    _super.call(this) || this;\n    _this.input = input;\n    return _this;\n    // End section: command_constructor\n  }\n  /**\n   * @internal\n   */\n  CreateDeliveryStreamCommand.prototype.resolveMiddleware = function (clientStack, configuration, options) {\n    this.middlewareStack.use(getSerdePlugin(configuration, this.serialize, this.deserialize));\n    var stack = clientStack.concat(this.middlewareStack);\n    var logger = configuration.logger;\n    var clientName = \"FirehoseClient\";\n    var commandName = \"CreateDeliveryStreamCommand\";\n    var handlerExecutionContext = {\n      logger: logger,\n      clientName: clientName,\n      commandName: commandName,\n      inputFilterSensitiveLog: CreateDeliveryStreamInput.filterSensitiveLog,\n      outputFilterSensitiveLog: CreateDeliveryStreamOutput.filterSensitiveLog\n    };\n    var requestHandler = configuration.requestHandler;\n    return stack.resolve(function (request) {\n      return requestHandler.handle(request.request, options || {});\n    }, handlerExecutionContext);\n  };\n  CreateDeliveryStreamCommand.prototype.serialize = function (input, context) {\n    return serializeAws_json1_1CreateDeliveryStreamCommand(input, context);\n  };\n  CreateDeliveryStreamCommand.prototype.deserialize = function (output, context) {\n    return deserializeAws_json1_1CreateDeliveryStreamCommand(output, context);\n  };\n  return CreateDeliveryStreamCommand;\n}($Command);\nexport { CreateDeliveryStreamCommand };","map":{"version":3,"sources":["../../../commands/CreateDeliveryStreamCommand.ts"],"names":[],"mappings":";AACA,SAAS,yBAAyB,EAAE,0BAA0B,QAAQ,oBAAoB;AAC1F,SACE,iDAAiD,EACjD,+CAA+C,QAC1C,0BAA0B;AACjC,SAAS,cAAc,QAAQ,2BAA2B;AAE1D,SAAS,OAAO,IAAI,QAAQ,QAAQ,wBAAwB;AAc5D;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAsEG;AACH,IAAA,2BAAA,GAAA,aAAA,UAAA,MAAA,EAAA;EAAiD,SAAA,CAAA,2BAAA,EAAA,MAAA,CAAA;EAK/C;EACA;EAEA,SAAA,2BAAA,CAAqB,KAAuC,EAAA;IAA5D,IAAA,KAAA;IACE;IACA,MAAA,CAAA,IAAA,CAAA,IAAA,CAAO,IAAA,IAAA;IAFY,KAAA,CAAA,KAAK,GAAL,KAAK;;IAGxB;EACF;EAEA;;AAEG;EACH,2BAAA,CAAA,SAAA,CAAA,iBAAiB,GAAjB,UACE,WAAmE,EACnE,aAA2C,EAC3C,OAA8B,EAAA;IAE9B,IAAI,CAAC,eAAe,CAAC,GAAG,CAAC,cAAc,CAAC,aAAa,EAAE,IAAI,CAAC,SAAS,EAAE,IAAI,CAAC,WAAW,CAAC,CAAC;IAEzF,IAAM,KAAK,GAAG,WAAW,CAAC,MAAM,CAAC,IAAI,CAAC,eAAe,CAAC;IAE9C,IAAA,MAAM,GAAK,aAAa,CAAA,MAAlB;IACd,IAAM,UAAU,GAAG,gBAAgB;IACnC,IAAM,WAAW,GAAG,6BAA6B;IACjD,IAAM,uBAAuB,GAA4B;MACvD,MAAM,EAAA,MAAA;MACN,UAAU,EAAA,UAAA;MACV,WAAW,EAAA,WAAA;MACX,uBAAuB,EAAE,yBAAyB,CAAC,kBAAkB;MACrE,wBAAwB,EAAE,0BAA0B,CAAC;KACtD;IACO,IAAA,cAAc,GAAK,aAAa,CAAA,cAAlB;IACtB,OAAO,KAAK,CAAC,OAAO,CAClB,UAAC,OAAsC,EAAA;MACrC,OAAA,cAAc,CAAC,MAAM,CAAC,OAAO,CAAC,OAAwB,EAAE,OAAO,IAAI,CAAA,CAAE,CAAC;IAAtE,CAAsE,EACxE,uBAAuB,CACxB;EACH,CAAC;EAEO,2BAAA,CAAA,SAAA,CAAA,SAAS,GAAjB,UAAkB,KAAuC,EAAE,OAAuB,EAAA;IAChF,OAAO,+CAA+C,CAAC,KAAK,EAAE,OAAO,CAAC;EACxE,CAAC;EAEO,2BAAA,CAAA,SAAA,CAAA,WAAW,GAAnB,UAAoB,MAAsB,EAAE,OAAuB,EAAA;IACjE,OAAO,iDAAiD,CAAC,MAAM,EAAE,OAAO,CAAC;EAC3E,CAAC;EAIH,OAAA,2BAAC;AAAD,CAAC,CAtDgD,QAAQ,CAAA","sourceRoot":"","sourcesContent":["import { __extends } from \"tslib\";\nimport { CreateDeliveryStreamInput, CreateDeliveryStreamOutput } from \"../models/models_0\";\nimport { deserializeAws_json1_1CreateDeliveryStreamCommand, serializeAws_json1_1CreateDeliveryStreamCommand, } from \"../protocols/Aws_json1_1\";\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\n/**\n * <p>Creates a Kinesis Data Firehose delivery stream.</p>\n *\n *          <p>By default, you can create up to 50 delivery streams per AWS Region.</p>\n *          <p>This is an asynchronous operation that immediately returns. The initial status of the\n *          delivery stream is <code>CREATING</code>. After the delivery stream is created, its status\n *          is <code>ACTIVE</code> and it now accepts data. If the delivery stream creation fails, the\n *          status transitions to <code>CREATING_FAILED</code>. Attempts to send data to a delivery\n *          stream that is not in the <code>ACTIVE</code> state cause an exception. To check the state\n *          of a delivery stream, use <a>DescribeDeliveryStream</a>.</p>\n *          <p>If the status of a delivery stream is <code>CREATING_FAILED</code>, this status\n *          doesn't change, and you can't invoke <code>CreateDeliveryStream</code> again on it.\n *          However, you can invoke the <a>DeleteDeliveryStream</a> operation to delete\n *          it.</p>\n *          <p>A Kinesis Data Firehose delivery stream can be configured to receive records directly\n *          from providers using <a>PutRecord</a> or <a>PutRecordBatch</a>, or it\n *          can be configured to use an existing Kinesis stream as its source. To specify a Kinesis\n *          data stream as input, set the <code>DeliveryStreamType</code> parameter to\n *             <code>KinesisStreamAsSource</code>, and provide the Kinesis stream Amazon Resource Name\n *          (ARN) and role ARN in the <code>KinesisStreamSourceConfiguration</code>\n *          parameter.</p>\n *          <p>To create a delivery stream with server-side encryption (SSE) enabled, include <a>DeliveryStreamEncryptionConfigurationInput</a> in your request. This is\n *          optional. You can also invoke <a>StartDeliveryStreamEncryption</a> to turn on\n *          SSE for an existing delivery stream that doesn't have SSE enabled.</p>\n *          <p>A delivery stream is configured with a single destination: Amazon S3, Amazon ES,\n *          Amazon Redshift, or Splunk. You must specify only one of the following destination\n *          configuration parameters: <code>ExtendedS3DestinationConfiguration</code>,\n *             <code>S3DestinationConfiguration</code>,\n *             <code>ElasticsearchDestinationConfiguration</code>,\n *             <code>RedshiftDestinationConfiguration</code>, or\n *             <code>SplunkDestinationConfiguration</code>.</p>\n *          <p>When you specify <code>S3DestinationConfiguration</code>, you can also provide the\n *          following optional values: BufferingHints, <code>EncryptionConfiguration</code>, and\n *             <code>CompressionFormat</code>. By default, if no <code>BufferingHints</code> value is\n *          provided, Kinesis Data Firehose buffers data up to 5 MB or for 5 minutes, whichever\n *          condition is satisfied first. <code>BufferingHints</code> is a hint, so there are some\n *          cases where the service cannot adhere to these conditions strictly. For example, record\n *          boundaries might be such that the size is a little over or under the configured buffering\n *          size. By default, no encryption is performed. We strongly recommend that you enable\n *          encryption to ensure secure data storage in Amazon S3.</p>\n *\n *          <p>A few notes about Amazon Redshift as a destination:</p>\n *          <ul>\n *             <li>\n *                <p>An Amazon Redshift destination requires an S3 bucket as intermediate location.\n *                Kinesis Data Firehose first delivers data to Amazon S3 and then uses\n *                   <code>COPY</code> syntax to load data into an Amazon Redshift table. This is\n *                specified in the <code>RedshiftDestinationConfiguration.S3Configuration</code>\n *                parameter.</p>\n *\n *             </li>\n *             <li>\n *                <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be\n *                specified in <code>RedshiftDestinationConfiguration.S3Configuration</code> because\n *                the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't\n *                support these compression formats.</p>\n *             </li>\n *             <li>\n *                <p>We strongly recommend that you use the user name and password you provide\n *                exclusively with Kinesis Data Firehose, and that the permissions for the account are\n *                restricted for Amazon Redshift <code>INSERT</code> permissions.</p>\n *\n *             </li>\n *          </ul>\n *          <p>Kinesis Data Firehose assumes the IAM role that is configured as part of the\n *          destination. The role should allow the Kinesis Data Firehose principal to assume the role,\n *          and the role should have permissions that allow the service to deliver the data. For more\n *          information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3\">Grant Kinesis Data\n *             Firehose Access to an Amazon S3 Destination</a> in the <i>Amazon Kinesis Data\n *             Firehose Developer Guide</i>.</p>\n */\nvar CreateDeliveryStreamCommand = /** @class */ (function (_super) {\n    __extends(CreateDeliveryStreamCommand, _super);\n    // Start section: command_properties\n    // End section: command_properties\n    function CreateDeliveryStreamCommand(input) {\n        var _this = \n        // Start section: command_constructor\n        _super.call(this) || this;\n        _this.input = input;\n        return _this;\n        // End section: command_constructor\n    }\n    /**\n     * @internal\n     */\n    CreateDeliveryStreamCommand.prototype.resolveMiddleware = function (clientStack, configuration, options) {\n        this.middlewareStack.use(getSerdePlugin(configuration, this.serialize, this.deserialize));\n        var stack = clientStack.concat(this.middlewareStack);\n        var logger = configuration.logger;\n        var clientName = \"FirehoseClient\";\n        var commandName = \"CreateDeliveryStreamCommand\";\n        var handlerExecutionContext = {\n            logger: logger,\n            clientName: clientName,\n            commandName: commandName,\n            inputFilterSensitiveLog: CreateDeliveryStreamInput.filterSensitiveLog,\n            outputFilterSensitiveLog: CreateDeliveryStreamOutput.filterSensitiveLog,\n        };\n        var requestHandler = configuration.requestHandler;\n        return stack.resolve(function (request) {\n            return requestHandler.handle(request.request, options || {});\n        }, handlerExecutionContext);\n    };\n    CreateDeliveryStreamCommand.prototype.serialize = function (input, context) {\n        return serializeAws_json1_1CreateDeliveryStreamCommand(input, context);\n    };\n    CreateDeliveryStreamCommand.prototype.deserialize = function (output, context) {\n        return deserializeAws_json1_1CreateDeliveryStreamCommand(output, context);\n    };\n    return CreateDeliveryStreamCommand;\n}($Command));\nexport { CreateDeliveryStreamCommand };\n//# sourceMappingURL=CreateDeliveryStreamCommand.js.map"]},"metadata":{},"sourceType":"module"}